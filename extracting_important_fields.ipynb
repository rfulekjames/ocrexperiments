{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import s3\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "data_bucket, input_bucket, output_bucket = '', ''\n",
    "prefix = ''\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "w_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "def collect_labeler_output_w(workflow):\n",
    "    # Update code to iterate over batches in workflow\n",
    "    # for each batch , run code below\n",
    "    print(f\"Collecting labeler output of all batches in workflow {workflow}\")\n",
    "    bucket = s3.Bucket(data_bucket)\n",
    "    # dont change code unless it works with previous workflow format\n",
    "    # now a workflow corresponds to a list of batches\n",
    "    # so this function takes as input a workflow, if workflow\n",
    "    # is a batch, same as before,\n",
    "    # w_dict is dictionary with keys workflows, values batches in them\n",
    "    # for batch in w_dict['workflow']\n",
    "    for b in w_dict[workflow]:\n",
    "        # collect batch labeler output and save\n",
    "        # Create dictionary for storing fields\n",
    "        true_value_fields = {f\"True Value {n+1}\": \"\" for n in range(7)}\n",
    "        change_reason_fields = {f\"Change Reason {n+1}\": \"\" for n in range(7)}\n",
    "        fields = {}\n",
    "        fields.update({\"RICOH_DCN\": []})\n",
    "        fields.update(true_value_fields)\n",
    "        fields.update(change_reason_fields)\n",
    "        fields.update(\n",
    "            {\n",
    "                \"ADDRESSEE\": \"\",\n",
    "                \"ADDRESS_LINE_1\": \"\",\n",
    "                \"ADDRESS_LINE_2\": \"\",\n",
    "                \"CITY\": \"\",\n",
    "                \"STATE\": \"\",\n",
    "                \"ZIP_CODE_4\": \"\",\n",
    "                \"REGULATORY_APPROVAL_ID\": \"\",\n",
    "                \"sub\": \"\",\n",
    "                \"submissionTime\": 0,\n",
    "                \"timeSpentInSeconds\": 0,\n",
    "                \"ADDRESSEE_confidence\": 0,\n",
    "                \"ADDRESS_LINE_1_confidence\": 0,\n",
    "                \"ADDRESS_LINE_2_confidence\": 0,\n",
    "                \"CITY_confidence\": 0,\n",
    "                \"STATE_confidence\": 0,\n",
    "                \"ZIP_CODE_4_confidence\": 0,\n",
    "                \"REGULATORY_APPROVAL_ID_confidence\": 0,\n",
    "            }\n",
    "        )\n",
    "        # Create dataframe schema (columns) for holding data\n",
    "        df = pd.DataFrame(fields)\n",
    "        # Search for objects belonging to this workflow\n",
    "        # for file in batch:\n",
    "        #    collect_data(file)\n",
    "\n",
    "        # List objects in the bucket with the given prefix\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=input_bucket, Prefix=f\"{prefix}/{b}\"\n",
    "        )\n",
    "\n",
    "        # Iterate over the objects and grab the filenames ending in .TIF\n",
    "        tif_files_in_batch = []\n",
    "        for obj in response[\"Contents\"]:\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\".TIF\"):\n",
    "                key = key.rsplit(\"/\")[-1]\n",
    "                tif_files_in_batch.append(key)\n",
    "\n",
    "        for x in tif_files_in_batch:\n",
    "            # for each file in batch, find corresponding output\n",
    "            for obj in bucket.objects.filter(Prefix=f\"a2i/output/{workflow}\"):\n",
    "                # print(f'Looking for: {x[:-4]}/output.json')\n",
    "                # print(f'Found: {obj.key}')\n",
    "                if obj.key.endswith(f\"{x[:-4]}/output.json\"):\n",
    "                    # print(f'This ends has desired ending: {obj.key}')\n",
    "                    # Load json output from S3\n",
    "                    content_object = s3.Object(bucket.name, obj.key)\n",
    "                    file_content = content_object.get()[\"Body\"].read().decode(\"utf-8\")\n",
    "                    json_content = json.loads(file_content)\n",
    "\n",
    "                    # need to get only the 9 digit filename that starts with C, with no file ending\n",
    "                    results = re.findall(r\"[C]\\d{9}\", obj.key)\n",
    "                    if results:\n",
    "                        ricoh_dcn = results[0]\n",
    "                    else:\n",
    "                        ricoh_dcn = obj.key\n",
    "\n",
    "                    # look up batch using ricoh\n",
    "                    # batch name = batch_dict['ricoh_dcn']\n",
    "                    # will be used below for\n",
    "                    # Dictionary to store data for this document\n",
    "                    data = fields.copy()\n",
    "                    # print(fields)\n",
    "\n",
    "                    # Add Ricoh_dcn\n",
    "                    data.update({\"RICOH_DCN\": ricoh_dcn})\n",
    "\n",
    "                    # NOTE: ConditionSatisfied is better called rules checked because even if not satisfied.\n",
    "                    # we get the rule appearing here.\n",
    "                    # store data from this document\n",
    "                    # Store the actual values returned by textract\n",
    "                    num_rules_checked = len(\n",
    "                        json_content[\"inputContent\"][\"Results\"][\"ConditionSatisfied\"]\n",
    "                    )\n",
    "                    for n in range(num_rules_checked):\n",
    "                        f = json_content[\"inputContent\"][\"Results\"][\n",
    "                            \"ConditionSatisfied\"\n",
    "                        ][n][\"field_name\"]\n",
    "                        v = json_content[\"inputContent\"][\"Results\"][\n",
    "                            \"ConditionSatisfied\"\n",
    "                        ][n][\"field_value\"]\n",
    "\n",
    "                        data[f] = v\n",
    "\n",
    "                    # Get actual confidence scores\n",
    "                    num_confidence_scores = len(\n",
    "                        json_content[\"inputContent\"][\"Results\"][\"ConditionMissed\"]\n",
    "                    )\n",
    "                    for n in range(num_confidence_scores):\n",
    "                        f_name = json_content[\"inputContent\"][\"Results\"][\n",
    "                            \"ConditionMissed\"\n",
    "                        ][n][\"field_name\"]\n",
    "                        f = f_name + \"_confidence\"\n",
    "                        msg = json_content[\"inputContent\"][\"Results\"][\n",
    "                            \"ConditionMissed\"\n",
    "                        ][n][\"message\"]\n",
    "                        r = re.compile(r\"\\d+\\.\\d*\")\n",
    "                        v = r.findall(msg)\n",
    "                        if not v:\n",
    "                            r = re.compile(r\"\\s+\\d+\")\n",
    "                            v = r.findall(msg)\n",
    "                        data[f] = v[0]\n",
    "\n",
    "                    # Corrections by labeler are 'humanAnswers'\n",
    "                    if \"humanAnswers\" in json_content:\n",
    "                        num_human_answers = len(json_content[\"humanAnswers\"])\n",
    "                        # print(num_human_answers)\n",
    "                    else:\n",
    "                        num_human_answers = 0\n",
    "                        # print('no human answers')\n",
    "\n",
    "                    # Get worker information about this particular labeling job\n",
    "                    for n in range(num_human_answers):\n",
    "                        # Get worker data\n",
    "                        data[\"submissionTime\"] = json_content[\"humanAnswers\"][n][\n",
    "                            \"submissionTime\"\n",
    "                        ]\n",
    "                        data[\"timeSpentInSeconds\"] = json_content[\"humanAnswers\"][n][\n",
    "                            \"timeSpentInSeconds\"\n",
    "                        ]\n",
    "                        data[\"sub\"] = json_content[\"humanAnswers\"][n][\"workerMetadata\"][\n",
    "                            \"identityData\"\n",
    "                        ][\"sub\"]\n",
    "\n",
    "                        # Get Answer information and add to data for this document\n",
    "                        answers = json_content[\"humanAnswers\"][n][\"answerContent\"]\n",
    "                        if answers:\n",
    "                            # add True Value to data\n",
    "                            for k, o in answers.items():\n",
    "                                data[k] = o\n",
    "                    df = df.append(data, ignore_index=True)\n",
    "\n",
    "        df.to_csv(\"labeler-output-\" + b + \".csv\", index=False)\n",
    "        s3_location = f\"{prefix}/{b}/output/labeler-output.csv\"\n",
    "        s3_client.upload_file(\n",
    "            \"./\" + \"labeler-output-\" + b + \".csv\", output_bucket, s3_location\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
